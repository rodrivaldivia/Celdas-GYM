{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tp-final-celdas.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa0Vpd8cj_Yf",
        "colab_type": "code",
        "outputId": "efa3040d-f422-42a6-c343-13af63995e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Note: If you haven't installed the following dependencies, run:\n",
        "!pip install 'imgaug==0.2.7'\n",
        "!git clone https://github.com/rubenrtorrado/GVGAI_GYM.git\n",
        "!pip install -e GVGAI_GYM\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imgaug==0.2.7 in /usr/local/lib/python3.6/dist-packages (0.2.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (3.1.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (2.4.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (1.6.4.post2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (6.2.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.7) (1.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.2.7) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.2.7) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.2.7) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.2.7) (0.10.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.7) (2.4)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.7) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug==0.2.7) (42.0.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.7) (4.4.1)\n",
            "fatal: destination path 'GVGAI_GYM' already exists and is not an empty directory.\n",
            "Obtaining file:///content/GVGAI_GYM\n",
            "Requirement already satisfied: gym>=0.10.5 in /usr/local/lib/python3.6/dist-packages (from gym-gvgai==0.0.3) (0.15.4)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from gym-gvgai==0.0.3) (1.17.4)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from gym-gvgai==0.0.3) (6.2.1)\n",
            "Requirement already satisfied: scipy<1.2.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from gym-gvgai==0.0.3) (1.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.5->gym-gvgai==0.0.3) (4.1.2.30)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.5->gym-gvgai==0.0.3) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.5->gym-gvgai==0.0.3) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.5->gym-gvgai==0.0.3) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym>=0.10.5->gym-gvgai==0.0.3) (0.16.0)\n",
            "Installing collected packages: gym-gvgai\n",
            "  Found existing installation: gym-gvgai 0.0.3\n",
            "    Can't uninstall 'gym-gvgai'. No files were found to uninstall.\n",
            "  Running setup.py develop for gym-gvgai\n",
            "Successfully installed gym-gvgai\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh_D1cQRvjL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19336dc7-c6e3-422a-8a81-0e4018275383"
      },
      "source": [
        "try:\n",
        "  %%tensorflow_version 2.x\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQrxOM30lLd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import gym_gvgai\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datetime as dt\n",
        "import os\n",
        "from random import randint\n",
        "from tensorflow import keras\n",
        "import random\n",
        "from scipy.spatial import distance\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzNTjUCfljc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MEMORY_CAPACITY = 50000  # @param {type:\"integer\"}\n",
        "state_size = 117  # @param {type:\"integer\"}\n",
        "NUM_ACTIONS = 5  # @param {type:\"integer\"}\n",
        "ALPHA = 0.001  # @param {type:\"number\"}\n",
        "batch_size = 32  # @param {type:\"integer\"}\n",
        "GAMMA = 0.95  # @param {type:\"number\"}\n",
        "TAU = 0.08  # @param {type:\"number\"}\n",
        "num_episodes = 100  # @param {type:\"integer\"}\n",
        "num_steps = 1000  # @param {type:\"integer\"}\n",
        "render = False # @param {type:\"boolean\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74aCgqUkl4HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STORE_PATH = os.getcwd()\n",
        "train_writer = tf.summary.create_file_writer(\n",
        "    STORE_PATH + \"/logs/Zelda_{}\".format(dt.datetime.now().strftime('%d%m%Y%H%M')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5N9cA7suYCW",
        "colab_type": "text"
      },
      "source": [
        "## Epsilon Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Oq3ygtubQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_EPSILON = 0.01\n",
        "\n",
        "class EpsilonStrategy():\n",
        "    def __init__(self):\n",
        "        self.epsilon = 1\n",
        "        self.epsilonDecreaseRate = 0.0001\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
        "\n",
        "    def shouldExploit(self):\n",
        "        if self.epsilon > MIN_EPSILON:\n",
        "            self.epsilon -= self.epsilonDecreaseRate\n",
        "        return random.uniform(0, 1) > self.epsilon\n",
        "\n",
        "    def epsilon(self):\n",
        "        return self.epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O82fN90Guc4o",
        "colab_type": "text"
      },
      "source": [
        "## Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJPYIB1Iufz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
        "\n",
        "    def pushExperience(self, experience):\n",
        "        self.memory.append(experience)\n",
        "        if len(self.memory) > self.capacity:\n",
        "            self.memory.pop(random.randint(0, len(self.memory) - 1))\n",
        "\n",
        "    def sample(self, batchSize):\n",
        "        if batchSize > len(self.memory):\n",
        "            return random.sample(self.memory, len(self.memory))\n",
        "        else:\n",
        "            return random.sample(self.memory, batchSize)\n",
        "\n",
        "    @property\n",
        "    def numSamples(self):\n",
        "        return len(self.memory)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAJfy5Jaui-7",
        "colab_type": "text"
      },
      "source": [
        "## Experience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXa895XdulJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Experience():\n",
        "    def __init__(self, state, actionIndex, reward, nextState):\n",
        "        self.state = state\n",
        "        self.actionIndex = actionIndex\n",
        "        self.reward = reward\n",
        "        self.nextState = nextState\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qIJdFYBurcR",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6EwdIxous-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self):\n",
        "        self.movementStrategy = EpsilonStrategy()\n",
        "        self.replayMemory = ReplayMemory(MEMORY_CAPACITY)\n",
        "        self.episode = 0\n",
        "\n",
        "        networkOptions = [\n",
        "            keras.layers.Dense(\n",
        "                state_size, input_dim=state_size, activation='relu'),\n",
        "            keras.layers.Dense(\n",
        "                100, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
        "            keras.layers.Dense(\n",
        "                100, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
        "            keras.layers.Dense(NUM_ACTIONS)\n",
        "        ]\n",
        "\n",
        "        self.policyNetwork = keras.Sequential(networkOptions)\n",
        "        self.targetNetwork = keras.Sequential(networkOptions)\n",
        "        self.policyNetwork.compile(optimizer=keras.optimizers.Adam(learning_rate=ALPHA),\n",
        "                                   loss=keras.losses.mean_squared_error)\n",
        "        print(self.policyNetwork.summary())\n",
        "        try:\n",
        "            self.policyNetwork.load_weights(\"./network/zelda-ddqn.h5\")\n",
        "            self.movementStrategy.epsilon = 0.01\n",
        "            print('Model loaded')\n",
        "        except:\n",
        "            print('Model file not found')\n",
        "\n",
        "    \"\"\"\n",
        "    * Public method to be called at the start of every level of a game.\n",
        "    * Perform any level-entry initialization here.\n",
        "    \"\"\"\n",
        "\n",
        "    def init(self):\n",
        "        self.lastState = None\n",
        "        self.lastPosition = None\n",
        "        self.lastActionIndex = None\n",
        "        self.averageLoss = 0\n",
        "        self.averageReward = 0\n",
        "        self.gameOver = False\n",
        "        self.cnt = 0\n",
        "        self.steps = 0\n",
        "        self.gotTheKey = False\n",
        "        self.keyPosition = None\n",
        "        self.closerToExit = False\n",
        "        self.closerToKey = False\n",
        "        print(\"Game initialized\")\n",
        "\n",
        "    def act(self, state):\n",
        "        # pprint(vars(state))\n",
        "        # pprint(state.NPCPositions)\n",
        "        # print(self.get_perception(state))\n",
        "        currentPosition = self.getAvatarCoordinates(state)\n",
        "        if not self.gotTheKey:\n",
        "            self.keyPosition = self.getKeyPosition(state)\n",
        "        self.exitPosition = self.getExitPosition(state)\n",
        "        if self.lastState is not None:\n",
        "            reward = self.getReward(self.lastState, currentPosition, state)\n",
        "            self.replayMemory.pushExperience(Experience(\n",
        "                self.lastState, self.lastActionIndex, reward, state))\n",
        "            # Train\n",
        "            loss = self.train()\n",
        "            self.averageLoss += loss\n",
        "            self.averageReward += reward\n",
        "\n",
        "        index = self.getNextAction(state)\n",
        "        # action = state.availableActions[index]\n",
        "        self.lastState = state\n",
        "        self.lastPosition = currentPosition\n",
        "        if index is not None:\n",
        "            self.lastActionIndex = index\n",
        "        # print(\"Action and index: \" + str(action) + \" \" + str(index))\n",
        "        self.steps += 1\n",
        "        return index\n",
        "\n",
        "    def getElementCoordinates(self, state, element):\n",
        "        result = None\n",
        "        for i in range(len(state)):\n",
        "            for j in range(len(state[0])):\n",
        "                if state[i][j] == element:\n",
        "                    result = [i, j]\n",
        "        return result\n",
        "\n",
        "    def getAvatarCoordinates(self, state):\n",
        "        return self.getElementCoordinates(state, 1.0)\n",
        "\n",
        "    def getKeyPosition(self, state):\n",
        "        return self.getElementCoordinates(state, 2.0)\n",
        "\n",
        "    def getExitPosition(self, state):\n",
        "        return self.getElementCoordinates(state, 3.0)\n",
        "\n",
        "    def getDistanceToKey(self, state):\n",
        "        return distance.cityblock(self.getAvatarCoordinates(state), self.keyPosition)\n",
        "\n",
        "    def getDistanceToExit(self, state):\n",
        "        return distance.cityblock(self.getAvatarCoordinates(state), self.exitPosition)\n",
        "\n",
        "    def isCloserToKey(self, previousState, currentState):\n",
        "        return self.getDistanceToKey(currentState) < self.getDistanceToKey(previousState)\n",
        "\n",
        "    def isCloserToExit(self, previousState, currentState):\n",
        "        return self.getDistanceToExit(currentState) < self.getDistanceToExit(previousState)\n",
        "\n",
        "    def getReward(self, lastState, currentPosition, currentState):\n",
        "        level = lastState\n",
        "        col = int(currentPosition[0])  # col\n",
        "        row = int(currentPosition[1])  # row\n",
        "        reward = 0.0\n",
        "        # if currentState.NPCPositionsNum < lastState.NPCPositionsNum:\n",
        "        #     print('KILLED AN ENEMY')\n",
        "        #     reward += 1.0\n",
        "        if self.keyPosition is not None and self.isCloserToKey(lastState, currentState):\n",
        "            reward += 3.0\n",
        "        # if not self.isCloserToKey(lastState, currentState):\n",
        "        #     reward += -1.0\n",
        "        # if self.gotTheKey and self.isCloserToExit(lastState, currentState):\n",
        "        #     print('Got the key and closer!')\n",
        "        #     reward += 2.0\n",
        "        if level[col][row] == 2.:\n",
        "            # If we got the key\n",
        "            print('GOT THE KEY')\n",
        "            self.gotTheKey = True\n",
        "            reward += 100.0\n",
        "        elif level[col][row] == 3. and self.gotTheKey:\n",
        "            # If we are at the exit\n",
        "            print('WON')\n",
        "            reward += 10000.0\n",
        "        elif level[col][row] == 4.:\n",
        "            # If we touched an enemy\n",
        "            reward += -10.0\n",
        "        elif level[col][row] == 0. or level[col][row] == 5.:\n",
        "            # If we are in a safe spot or didn't move\n",
        "            reward += -5.0\n",
        "        return reward\n",
        "\n",
        "        # Modify here to alter network inputs, be careful of dynamic arrays and to change network inputs\n",
        "    def buildNetworkInput(self, state):\n",
        "        perception = []\n",
        "        perception = np.append(\n",
        "            perception, np.ravel(state))\n",
        "        # perception = np.append(perception, state.gameScore)\n",
        "        # perception = np.append(perception, 0.0 if state.isGameOver else 1.0)\n",
        "        # perception = np.append(perception, 0.0 if not self.gotTheKey else 1.0)\n",
        "        # perception = np.append(perception, 0.0 if not self.closerToExit else 1.0)\n",
        "        # perception = np.append(perception, 0.0 if not self.closerToKey else 1.0)\n",
        "        # perception = np.append(perception, actionToFloat[state.avatarLastAction])\n",
        "        # perception = np.append(perception, np.ravel(state.avatarOrientation))\n",
        "        # perception = np.append(perception, len(state.NPCPositions)) # number of enemies\n",
        "        # perception = np.append(perception, np.ravel([i.getPositionAsArray() for i in np.ravel(state.portalsPositions)]))\n",
        "        # perception = np.append(perception, np.ravel(\n",
        "        # [i.getPositionAsArray() for i in np.ravel(state.NPCPositions)]))\n",
        "        # perception = np.append(perception, self.getDistanceToKey(state))\n",
        "        # perception = np.append(perception, self.getDistanceToExit(state))\n",
        "        # perception = np.append(perception, np.ravel(\n",
        "        #     [i.getPositionAsArray() for i in np.ravel(state.resourcesPositions)]))\n",
        "        return perception\n",
        "\n",
        "    def getNextAction(self, state):\n",
        "        # Do exploration or exploitation\n",
        "        if self.movementStrategy.shouldExploit():\n",
        "            #Exploitation\n",
        "            # print('Exploitation')\n",
        "            sd = tf.reshape(self.policyNetwork(tf.convert_to_tensor(\n",
        "                [self.buildNetworkInput(state)], dtype=tf.float32)), (1, -1))\n",
        "            return np.argmax(sd)\n",
        "        else:\n",
        "            #Exploration\n",
        "            # print('Exploration')\n",
        "            return random.randint(0, NUM_ACTIONS - 1)\n",
        "\n",
        "    def train(self):\n",
        "        if self.replayMemory.numSamples < batch_size * 3:\n",
        "            return 0\n",
        "        batch = self.replayMemory.sample(batch_size)\n",
        "        # rawStates = [np.ravel(self.get_perception(val.state)) for val in batch]\n",
        "        rawStates = [self.buildNetworkInput(val.state) for val in batch]\n",
        "        states = tf.convert_to_tensor(rawStates, dtype=tf.float32)\n",
        "        actions = np.array([val.actionIndex for val in batch])\n",
        "        rewards = np.array([val.reward for val in batch])\n",
        "        rawNextStates = [\n",
        "            (np.zeros(state_size) if val.nextState is None else self.buildNetworkInput(val.nextState)) for val in batch]\n",
        "        # rawNextStates = [(np.zeros(state_size) if val.nextState is None else val.nextState) for val in batch]\n",
        "        # preTensorNextStates = [self.buildNetworkInput(val.nextState) for val in rawNextStates]\n",
        "        nextStates = tf.convert_to_tensor(rawNextStates, dtype=tf.float32)\n",
        "        # predict Q(s,a) given the batch of states\n",
        "        prim_qt = self.policyNetwork(states)\n",
        "        # predict Q(s',a') from the evaluation network\n",
        "        prim_qtp1 = self.policyNetwork(nextStates)\n",
        "        # copy the prim_qt into the target_q tensor - we then will update one index corresponding to the max action\n",
        "        target_q = prim_qt.numpy()\n",
        "        updates = rewards\n",
        "        valid_idxs = np.array(nextStates).sum(axis=1) != 0\n",
        "        batch_idxs = np.arange(batch_size)\n",
        "\n",
        "        prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
        "        q_from_target = self.targetNetwork(nextStates)\n",
        "        updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs],\n",
        "                                                             prim_action_tp1[valid_idxs]]\n",
        "        target_q[batch_idxs, actions] = updates\n",
        "        loss = self.policyNetwork.train_on_batch(states, target_q)\n",
        "        # update target network parameters slowly from policy network\n",
        "        for t, e in zip(self.targetNetwork.trainable_variables, self.policyNetwork.trainable_variables):\n",
        "            t.assign(t * (1 - TAU) + e * TAU)\n",
        "        return loss\n",
        "\n",
        "    def result(self, sso, gameWinner):\n",
        "        self.gameOver = True\n",
        "        reward = 0\n",
        "        if self.lastActionIndex is not None:\n",
        "            if gameWinner == 'PLAYER_LOSES':\n",
        "                reward += -10.0\n",
        "            elif gameWinner == 'PLAYER_WINS':\n",
        "                reward += 10000.0\n",
        "            self.replayMemory.pushExperience(Experience(\n",
        "                self.lastState, self.lastActionIndex, reward, sso))\n",
        "        self.episode += 1\n",
        "\n",
        "        if self.gameOver:\n",
        "            self.averageLoss /= self.steps\n",
        "            print(\"Episode: {}, Reward: {}, avg loss: {}, eps: {}\".format(\n",
        "                self.episode, self.averageReward, self.averageLoss, self.movementStrategy.epsilon))\n",
        "            print(\"Winner: {}\".format(gameWinner))\n",
        "            with train_writer.as_default():\n",
        "                tf.summary.scalar(\n",
        "                    'reward', self.averageReward, step=self.steps)\n",
        "                tf.summary.scalar(\n",
        "                    'avg loss', self.averageLoss, step=self.steps)\n",
        "        if self.episode % 10 == 0:\n",
        "            self.policyNetwork.save_weights(\"./network/zelda-ddqn.h5\")\n",
        "            print('Model saved!')\n",
        "        return random.randint(0, 2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkJj79Qsu07p",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2QyNkvzu2s1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9afd0bc2-a043-4ebb-d3fc-8fec608e03f5"
      },
      "source": [
        "env_name = 'gvgai-zelda-lvl0-v0'\n",
        "env = gym_gvgai.make(env_name)\n",
        "\n",
        "print('Starting ' + env.env.game + \" with Level \" + str(env.env.lvl))\n",
        "\n",
        "\n",
        "def grayToArray(array):\n",
        "\tresult = np.zeros((9, 13))\n",
        "\tfor i in range(int(array.shape[0]/10)):\n",
        "\t\tfor j in range(int(array.shape[1]/10)):\n",
        "\t\t\tresult[i][j] = int(array[10*i+5, 10*j+5])\n",
        "\t\t\telem = result[i][j]\n",
        "\t\t\tif elem == 53:  # Empty\n",
        "\t\t\t\tresult[i][j] = 0.0\n",
        "\t\t\tif elem == 201 or elem == 38:  # Avatar\n",
        "\t\t\t\tresult[i][j] = 1.0\n",
        "\t\t\tif elem == 123:  # Key\n",
        "\t\t\t\tresult[i][j] = 2.0\n",
        "\t\t\tif elem == 52:  # Door\n",
        "\t\t\t\tresult[i][j] = 3.0\n",
        "\t\t\tif elem == 61:  # Enemy\n",
        "\t\t\t\tresult[i][j] = 4.0\n",
        "\t\t\tif elem == 127 or elem == 92:  # Wall\n",
        "\t\t\t\tresult[i][j] = 5.0\n",
        "\n",
        "\treturn result\n",
        "\n",
        "def getState():\n",
        "    rgb = env.render('rgb_array')\n",
        "    gray = np.mean(rgb, -1)\n",
        "    return grayToArray(gray)\n",
        "\n",
        "agent = Agent()\n",
        "\n",
        "for i in range(num_episodes):  # testing 100 times\n",
        "    current_score = 0  # record current testing round score\n",
        "    env.reset()\n",
        "    state = getState()\n",
        "    agent.init()\n",
        "    for step in range(num_steps):\n",
        "        if render:\n",
        "          env.render()\n",
        "        action = agent.act(state)\n",
        "        stateObs, increScore, done, debug = env.step(action)\n",
        "        state = getState()\n",
        "        if done:\n",
        "            agent.result(state, debug['winner'])\n",
        "            print(\"Game over at game tick \" + str(step+1) + \" with player \" + debug['winner'])\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Connecting to host 127.0.0.1 at port 39743 ...\n",
            "Client connected to server [OK]\n",
            "Starting zelda with Level 0\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 117)               13806     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               11800     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 5)                 505       \n",
            "=================================================================\n",
            "Total params: 36,211\n",
            "Trainable params: 36,211\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model file not found\n",
            "Game initialized\n",
            "Episode: 1, Reward: -17.0, avg loss: 0.4882300578314683, eps: 0.9884000000000013\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 116 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 2, Reward: -39.0, avg loss: 0.6260362575451534, eps: 0.9764000000000026\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 120 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 3, Reward: -73.0, avg loss: 2.4025590759289415, eps: 0.9527000000000052\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 237 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 4, Reward: -35.0, avg loss: 1.1522594456906086, eps: 0.9384000000000068\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 143 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 5, Reward: -63.0, avg loss: 1.2385642386972904, eps: 0.918400000000009\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 200 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 6, Reward: -44.0, avg loss: 0.8605455238303524, eps: 0.9021000000000108\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 163 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 7, Reward: -32.0, avg loss: 0.7913451738316905, eps: 0.8889000000000122\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 132 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 8, Reward: -8.0, avg loss: 0.8312099562449888, eps: 0.8845000000000127\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 44 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 9, Reward: -55.0, avg loss: 0.6314842756837606, eps: 0.8661000000000147\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 184 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 10, Reward: -92.0, avg loss: 0.8223420437721033, eps: 0.8409000000000175\n",
            "Winner: PLAYER_LOSES\n",
            "Model saved!\n",
            "Game over at game tick 252 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 11, Reward: 4.0, avg loss: 0.3069052484474684, eps: 0.8390000000000177\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 19 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 12, Reward: 0.0, avg loss: 0.45187607490354115, eps: 0.8372000000000179\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 18 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 13, Reward: -31.0, avg loss: 0.45195602738496027, eps: 0.8240000000000194\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 132 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 14, Reward: -39.0, avg loss: 0.43623260828524324, eps: 0.800400000000022\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 236 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 15, Reward: -96.0, avg loss: 0.39632024849441594, eps: 0.7621000000000262\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 383 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 16, Reward: -18.0, avg loss: 0.4265394454919978, eps: 0.7393000000000287\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 228 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 17, Reward: -10.0, avg loss: 0.3557021745035182, eps: 0.7301000000000297\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 92 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 18, Reward: -21.0, avg loss: 0.33095498098170056, eps: 0.7165000000000312\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 136 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 19, Reward: -34.0, avg loss: 0.5085439825546928, eps: 0.6845000000000347\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 320 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 20, Reward: -6.0, avg loss: 0.5064396425372079, eps: 0.6803000000000352\n",
            "Winner: PLAYER_LOSES\n",
            "Model saved!\n",
            "Game over at game tick 42 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 21, Reward: -17.0, avg loss: 0.4238715253657464, eps: 0.6671000000000367\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 132 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 22, Reward: -32.0, avg loss: 0.3910589353981042, eps: 0.6473000000000388\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 198 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 23, Reward: -12.0, avg loss: 0.37340477972545405, eps: 0.6253000000000413\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 220 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "GOT THE KEY\n",
            "Episode: 24, Reward: -62.0, avg loss: 1.3244509408868756, eps: 0.5289000000000519\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 964 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 25, Reward: 0.0, avg loss: 0.30526777539545524, eps: 0.518500000000053\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 104 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 26, Reward: -20.0, avg loss: 0.3239915000481738, eps: 0.5149000000000534\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 36 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Game initialized\n",
            "GOT THE KEY\n",
            "Game initialized\n",
            "Episode: 27, Reward: -29.0, avg loss: 1.1057891896742054, eps: 0.2997000000000771\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 152 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "GOT THE KEY\n",
            "Episode: 28, Reward: 5.0, avg loss: 0.7514100337095184, eps: 0.23210000000008457\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 676 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 29, Reward: -8.0, avg loss: 0.21762477156880777, eps: 0.22570000000008528\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 64 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 30, Reward: -82.0, avg loss: 2.040082244133581, eps: 0.1784000000000905\n",
            "Winner: PLAYER_LOSES\n",
            "Model saved!\n",
            "Game over at game tick 473 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 31, Reward: -12.0, avg loss: 0.4229981540120207, eps: 0.1384000000000949\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 400 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 32, Reward: -15.0, avg loss: 1.6121680471647, eps: 0.12440000000009635\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 140 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 33, Reward: -8.0, avg loss: 1.0054575621404431, eps: 0.1156000000000961\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 88 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 34, Reward: -38.0, avg loss: 1.1924953232758129, eps: 0.0844000000000952\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 312 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 35, Reward: -32.0, avg loss: 0.8278498124212114, eps: 0.05000000000009422\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 344 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 36, Reward: -12.0, avg loss: 0.4182603179047314, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 728 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 37, Reward: -10.0, avg loss: 1.0616030857789438, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 77 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 38, Reward: -29.0, avg loss: 0.5279977100328184, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 260 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 39, Reward: -9.0, avg loss: 0.9039321224059377, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 776 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 40, Reward: -8.0, avg loss: 1.1307474209784265, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Model saved!\n",
            "Game over at game tick 137 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Game initialized\n",
            "Episode: 41, Reward: -1.0, avg loss: 2.205541609582724, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 64 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 42, Reward: -49.0, avg loss: 1.0894480617996305, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 800 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 43, Reward: -4.0, avg loss: 0.26680218129982486, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 152 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 44, Reward: -27.0, avg loss: 0.4373132215541658, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 316 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 45, Reward: -12.0, avg loss: 1.1623199638450825, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 948 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 46, Reward: -16.0, avg loss: 1.579266876889417, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 308 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 47, Reward: -25.0, avg loss: 1.3254341687198128, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 472 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 48, Reward: -8.0, avg loss: 1.125321888180003, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 592 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 49, Reward: -14.0, avg loss: 0.9256762264994904, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 332 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 50, Reward: 12.0, avg loss: 0.6252876548660037, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Model saved!\n",
            "Game over at game tick 752 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 51, Reward: 4.0, avg loss: 0.36965175679969514, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 44 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 52, Reward: 3.0, avg loss: 0.1960436131154314, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 352 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 53, Reward: -1.0, avg loss: 2.4461557888598353, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 27 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 54, Reward: -10.0, avg loss: 0.20774390842832094, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 56 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 55, Reward: 12.0, avg loss: 0.38029922744262185, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 316 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 56, Reward: 12.0, avg loss: 0.1652071093623009, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 36 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 57, Reward: 21.0, avg loss: 0.652158448741885, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 524 with player PLAYER_LOSES\n",
            "Game initialized\n",
            "Episode: 58, Reward: 8.0, avg loss: 0.20488284360617398, eps: 0.009900000000093813\n",
            "Winner: PLAYER_LOSES\n",
            "Game over at game tick 40 with player PLAYER_LOSES\n",
            "Game initialized\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}